{
  "name": "distributed-llama-api",
  "version": "1.0.0",
  "description": "Distributed Llama inference API with WebSocket support and load balancing",
  "main": "server.js",
  "scripts": {
    "start": "node server.js",
    "dev": "nodemon server.js",
    "test": "jest --coverage",
    "test:e2e": "jest --config=jest.e2e.config.js"
  },
  "dependencies": {
    "ws": "^8.14.2",
    "express": "^4.18.2",
    "cors": "^2.8.5",
    "ioredis": "^5.3.2",
    "node-fetch": "^2.7.0"
  },
  "devDependencies": {
    "nodemon": "^3.0.1",
    "jest": "^29.7.0",
    "@types/ws": "^8.5.8",
    "@types/express": "^4.17.20"
  },
  "engines": {
    "node": ">=14.0.0"
  },
  "author": "Sally (UX Expert)",
  "license": "MIT"
}