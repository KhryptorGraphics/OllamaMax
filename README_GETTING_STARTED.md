# üöÄ OllamaMax Getting Started Guide

Welcome to OllamaMax! This guide will get you up and running in just a few minutes.

## Quick Start (60 seconds)

For the fastest setup experience, use our quickstart command:

```bash
# Install OllamaMax
curl -fsSL https://raw.githubusercontent.com/KhryptorGraphics/OllamaMax/main/scripts/install.sh | bash

# Start immediately with defaults
ollama-distributed quickstart
```

That's it! üéâ Your OllamaMax node is now running with:
- **Web Dashboard**: http://localhost:8081
- **API Endpoint**: http://localhost:8080
- **Health Check**: http://localhost:8080/health

## Installation Options

### 1. Automatic Installation (Recommended)

```bash
# Universal installer (Linux, macOS, Windows WSL)
curl -fsSL https://install.ollamamax.com | bash

# Or with options
curl -fsSL https://install.ollamamax.com | bash -s -- --enable-gpu --quick
```

### 2. Manual Installation

```bash
# Download and run installer
wget https://github.com/KhryptorGraphics/OllamaMax/raw/main/scripts/install.sh
chmod +x install.sh
./install.sh --help
```

### 3. From Source

```bash
git clone https://github.com/KhryptorGraphics/OllamaMax.git
cd OllamaMax
go build -o ollama-distributed ./ollama-distributed/cmd/node
```

## Getting Started Workflows

### üèÉ‚Äç‚ôÇÔ∏è Speed Runner (1 minute)
```bash
ollama-distributed quickstart
# ‚úÖ Node running, web UI open, ready to go!
```

### üõ†Ô∏è Custom Setup (5 minutes)
```bash
ollama-distributed setup
# Interactive wizard guides you through configuration
# Choose ports, enable features, configure security
```

### üîß Advanced Setup (15 minutes)
```bash
# Generate custom configuration
./scripts/config-generator.sh --profile production --security

# Validate configuration
ollama-distributed validate --config production-config.yaml

# Start with custom config
ollama-distributed start --config production-config.yaml
```

## Your First AI Conversation

After starting OllamaMax, try these commands:

```bash
# Download a model
ollama-distributed proxy pull phi3:mini

# Chat via API
curl -X POST http://localhost:8080/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "model": "phi3:mini",
    "messages": [{"role": "user", "content": "Hello! How are you?"}]
  }'

# Or use the beautiful web interface
open http://localhost:8081
```

## Essential Commands

| Command | Purpose | Example |
|---------|---------|---------|
| `quickstart` | Instant setup with defaults | `ollama-distributed quickstart` |
| `setup` | Interactive configuration wizard | `ollama-distributed setup` |
| `start` | Start the OllamaMax node | `ollama-distributed start` |
| `status` | Check system health | `ollama-distributed status --verbose` |
| `validate` | Validate configuration | `ollama-distributed validate --fix` |
| `proxy pull` | Download AI models | `ollama-distributed proxy pull llama2:7b` |
| `examples` | See usage examples | `ollama-distributed examples` |
| `tutorial` | Interactive tutorial | `ollama-distributed tutorial` |
| `troubleshoot` | Fix common issues | `ollama-distributed troubleshoot` |

## Configuration Profiles

Choose the right profile for your use case:

### üîß Development
```bash
./scripts/config-generator.sh --profile development
# ‚Ä¢ Debug logging, minimal security
# ‚Ä¢ Perfect for testing and development
# ‚Ä¢ Single node, low resource usage
```

### üè≠ Production  
```bash
./scripts/config-generator.sh --profile production --security
# ‚Ä¢ Optimized performance, security enabled
# ‚Ä¢ Structured logging, resource limits
# ‚Ä¢ Production-ready configuration
```

### üåê Cluster
```bash
./scripts/config-generator.sh --profile cluster --gpu
# ‚Ä¢ Multi-node distributed setup  
# ‚Ä¢ P2P networking, automatic failover
# ‚Ä¢ High availability and scalability
```

### ‚ö° GPU-Accelerated
```bash
./scripts/config-generator.sh --profile gpu --enable-gpu
# ‚Ä¢ GPU acceleration enabled
# ‚Ä¢ Optimized for AI/ML workloads
# ‚Ä¢ Higher performance limits
```

## Integration with Existing Ollama

Already have Ollama installed? We've got you covered:

```bash
# Scan for existing installations
./scripts/ollama-integration.sh --scan

# Migrate your existing models and data
./scripts/ollama-integration.sh --mode migrate

# Run alongside existing Ollama (different ports)
./scripts/ollama-integration.sh --mode coexist
```

## Web Interface Features

Open http://localhost:8081 to access:

- üí¨ **Chat Interface** - Interactive AI conversations
- ü§ñ **Model Manager** - Download, update, and manage models  
- üìä **Performance Dashboard** - Real-time metrics and monitoring
- ‚öôÔ∏è **Configuration** - Visual configuration management
- üîß **System Tools** - Diagnostics and maintenance
- üìö **API Explorer** - Test and explore REST APIs

## Troubleshooting

### Quick Fixes

```bash
# Check if everything is working
ollama-distributed status

# Fix common configuration issues  
ollama-distributed validate --fix

# Reset to defaults if needed
ollama-distributed quickstart --force

# Get diagnostic information
ollama-distributed troubleshoot
```

### Common Issues

#### Port Already in Use
```bash
# Use different ports
ollama-distributed setup
# Choose custom ports during setup

# Or specify ports directly  
ollama-distributed start --api-port 8090 --web-port 8091
```

#### Models Not Loading
```bash
# Check model directory
ollama-distributed status --verbose

# Re-download models
ollama-distributed proxy pull phi3:mini --force

# Check disk space
df -h
```

#### Permission Errors
```bash
# Fix data directory permissions
sudo chown -R $USER:$USER ~/.ollamamax

# Or use custom directory
ollama-distributed setup --data-dir ~/my-ollama-data
```

## Next Steps

### üìö Learn More
- [Complete Tutorial](https://docs.ollamamax.com/tutorial) - Step-by-step learning
- [API Documentation](https://docs.ollamamax.com/api) - REST API reference  
- [Configuration Guide](https://docs.ollamamax.com/config) - Advanced configuration
- [Deployment Guide](https://docs.ollamamax.com/deploy) - Production deployment

### üöÄ Scale Up
- [Cluster Setup](https://docs.ollamamax.com/cluster) - Multi-node deployment
- [Performance Tuning](https://docs.ollamamax.com/performance) - Optimization guide
- [Monitoring](https://docs.ollamamax.com/monitoring) - Metrics and alerting
- [Security](https://docs.ollamamax.com/security) - Production security

### ü§ù Community
- [GitHub](https://github.com/KhryptorGraphics/OllamaMax) - Source code and issues
- [Discussions](https://github.com/KhryptorGraphics/OllamaMax/discussions) - Community forum
- [Discord](https://discord.gg/ollamamax) - Real-time chat
- [Examples](https://github.com/KhryptorGraphics/OllamaMax/examples) - Sample applications

## Support

Need help? We're here for you:

- üìñ **Documentation**: https://docs.ollamamax.com
- üêõ **Bug Reports**: https://github.com/KhryptorGraphics/OllamaMax/issues
- üí¨ **Community**: https://github.com/KhryptorGraphics/OllamaMax/discussions
- üìß **Email**: support@ollamamax.com

---

**Welcome to the future of distributed AI!** üåü

Ready to get started? Run: `ollama-distributed quickstart`