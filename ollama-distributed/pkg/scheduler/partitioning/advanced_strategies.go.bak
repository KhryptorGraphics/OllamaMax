package partitioning

import (
	"context"
	"fmt"
	"math"
	"time"

	"github.com/ollama/ollama/api"
	"github.com/ollama/ollama/fs/ggml"
	"github.com/ollama/ollama/server"
	"github.com/ollama/ollama/discover"
)

// PipelineParallelismStrategy implements pipeline parallelism for sequential models
type PipelineParallelismStrategy struct {
	metrics *StrategyMetrics
}

// NewPipelineParallelismStrategy creates a new pipeline parallelism strategy
func NewPipelineParallelismStrategy() *PipelineParallelismStrategy {
	return &PipelineParallelismStrategy{
		metrics: &StrategyMetrics{
			LastUsed: time.Now(),
		},
	}
}

// GetName returns the strategy name
func (pps *PipelineParallelismStrategy) GetName() string {
	return "pipeline_parallel"
}

// GetMetrics returns strategy metrics
func (pps *PipelineParallelismStrategy) GetMetrics() *StrategyMetrics {
	return pps.metrics
}

// CanHandle checks if this strategy can handle the task
func (pps *PipelineParallelismStrategy) CanHandle(task *PartitionTask) bool {
	// This strategy works well for sequential models with many layers
	if task.GGML != nil {
		kv := task.GGML.KV()
		if layers := kv.Uint("llm.layers"); layers > 20 { // Only for models with many layers
			return true
		}
	}
	return false
}

// Partition implements pipeline parallelism partitioning
func (pps *PipelineParallelismStrategy) Partition(ctx context.Context, task *PartitionTask) (*PartitionPlan, error) {
	start := time.Now()
	
	// Get number of layers
	layerCount := 0
	if task.GGML != nil {
		kv := task.GGML.KV()
		if layers := kv.Uint("llm.layers"); layers > 0 {
			layerCount = int(layers)
		}
	}
	
	if layerCount == 0 {
		return nil, fmt.Errorf("unable to determine layer count")
	}
	
	// Determine pipeline stages
	nodeCount := len(task.Nodes)
	if nodeCount == 0 {
		return nil, fmt.Errorf("no nodes available")
	}
	
	// Calculate layers per stage
	layersPerStage := int(math.Ceil(float64(layerCount) / float64(nodeCount)))
	
	// Create partitions
	partitions := make([]*Partition, 0)
	stageIndex := 0
	
	for i := 0; i < layerCount; i += layersPerStage {
		end := i + layersPerStage
		if end > layerCount {
			end = layerCount
		}
		
		// Assign to node in round-robin fashion
		nodeIndex := stageIndex % nodeCount
		nodeID := task.Nodes[nodeIndex].ID
		
		partition := &Partition{
			ID:          fmt.Sprintf("partition_%d_stage_%d", task.ID, stageIndex),
			NodeID:      nodeID,
			Type:        PartitionTypeLayer,
			Data: map[string]interface{}{
				"start_layer": i,
				"end_layer":   end,
				"layer_count": end - i,
			},
			Dependencies: []string{}, // Will be set later
			Metadata: map[string]interface{}{
				"stage": stageIndex,
			},
			EstimatedLatency: time.Duration((end-i)*10) * time.Millisecond, // Rough estimate
			EstimatedMemory:  int64((end - i) * 100 * 1024 * 1024),       // Rough estimate (100MB per layer)
		}
		
		// Set dependencies (each stage depends on the previous one)
		if stageIndex > 0 {
			partition.Dependencies = append(partition.Dependencies, fmt.Sprintf("partition_%d_stage_%d", task.ID, stageIndex-1))
		}
		
		partitions = append(partitions, partition)
		stageIndex++
	}
	
	// Create plan
	plan := &PartitionPlan{
		ID:           fmt.Sprintf("plan_%s", task.ID),
		Strategy:     pps.GetName(),
		Partitions:   partitions,
		Metadata:     make(map[string]interface{}),
		CreatedAt:    time.Now(),
		EstimatedLatency: time.Duration(layerCount*10) * time.Millisecond,
		EstimatedThroughput: 1.0, // Placeholder
		OptimizationScore: 0.8,   // Placeholder
	}
	
	// Update metrics
	pps.metrics.TotalPartitions += int64(len(partitions))
	pps.metrics.SuccessfulPartitions += int64(len(partitions))
	pps.metrics.LastUsed = time.Now()
	pps.metrics.AverageLatency = (pps.metrics.AverageLatency*time.Duration(pps.metrics.SuccessfulPartitions-1) + 
		time.Since(start)) / time.Duration(pps.metrics.SuccessfulPartitions)
	
	return plan, nil
}

// TensorParallelismStrategy implements tensor parallelism for intra-layer operations
type TensorParallelismStrategy struct {
	metrics *StrategyMetrics
}

// NewTensorParallelismStrategy creates a new tensor parallelism strategy
func NewTensorParallelismStrategy() *TensorParallelismStrategy {
	return &TensorParallelismStrategy{
		metrics: &StrategyMetrics{
			LastUsed: time.Now(),
		},
	}
}

// GetName returns the strategy name
func (tps *TensorParallelismStrategy) GetName() string {
	return "tensor_parallel"
}

// GetMetrics returns strategy metrics
func (tps *TensorParallelismStrategy) GetMetrics() *StrategyMetrics {
	return tps.metrics
}

// CanHandle checks if this strategy can handle the task
func (tps *TensorParallelismStrategy) CanHandle(task *PartitionTask) bool {
	// This strategy works for models with large tensors that can be split
	// For now, we'll use a simple heuristic based on context length in options
	contextLength := 0
	
	// Try to get context length from NumCtx in Runner
	if task.Options.NumCtx > 0 {
		contextLength = task.Options.NumCtx
	}
	
	// Try to get context length from Options map if available
	if contextLength == 0 {
		if optsMap, ok := task.Options.Options.(map[string]interface{}); ok {
			if ctxLen, exists := optsMap["num_ctx"]; exists {
				if ctxLenInt, ok := ctxLen.(int); ok {
					contextLength = ctxLenInt
				}
			}
		}
	}
	
	// Only for large context models
	return contextLength > 2048
}

// Partition implements tensor parallelism partitioning
func (tps *TensorParallelismStrategy) Partition(ctx context.Context, task *PartitionTask) (*PartitionPlan, error) {
	start := time.Now()
	
	// Get context length
	contextLength := 0
	if task.Options.NumCtx > 0 {
		contextLength = task.Options.NumCtx
	} else if optsMap, ok := task.Options.Options.(map[string]interface{}); ok {
		if ctxLen, exists := optsMap["num_ctx"]; exists {
			if ctxLenInt, ok := ctxLen.(int); ok {
				contextLength = ctxLenInt
			}
		}
	}
	
	if contextLength == 0 {
		contextLength = 2048 // Default context length
	}
	
	nodeCount := len(task.Nodes)
	if nodeCount == 0 {
		return nil, fmt.Errorf("no nodes available")
	}
	
	// For tensor parallelism, we split the computation across nodes
	// rather than splitting layers
	partitions := make([]*Partition, nodeCount)
	
	// Split the context across nodes
	contextPerNode := contextLength / nodeCount
	remainder := contextLength % nodeCount
	
	for i := 0; i < nodeCount; i++ {
		startToken := i * contextPerNode
		endToken := startToken + contextPerNode
		if i < remainder {
			startToken += i
			endToken += i + 1
		} else {
			startToken += remainder
			endToken += remainder
		}
		
		partition := &Partition{
			ID:     fmt.Sprintf("partition_%d_tensor_%d", task.ID, i),
			NodeID: task.Nodes[i].ID,
			Type:   PartitionTypeData,
			Data: map[string]interface{}{
				"start_token": startToken,
				"end_token":   endToken,
				"token_count": endToken - startToken,
			},
			Dependencies: []string{}, // All partitions can run in parallel
			Metadata: map[string]interface{}{
				"tensor_split": i,
			},
			EstimatedLatency: time.Duration((endToken-startToken)*5) * time.Millisecond,
			EstimatedMemory:  int64((endToken - startToken) * 2 * 1024), // Rough estimate (2KB per token)
		}
		
		partitions[i] = partition
	}
	
	// Create plan
	plan := &PartitionPlan{
		ID:           fmt.Sprintf("plan_%s", task.ID),
		Strategy:     tps.GetName(),
		Partitions:   partitions,
		Metadata:     make(map[string]interface{}),
		CreatedAt:    time.Now(),
		EstimatedLatency: time.Duration(contextLength*5) * time.Millisecond / time.Duration(nodeCount),
		EstimatedThroughput: float64(nodeCount), // Placeholder
		OptimizationScore: 0.7,                 // Placeholder
	}
	
	// Update metrics
	tps.metrics.TotalPartitions += int64(len(partitions))
	tps.metrics.SuccessfulPartitions += int64(len(partitions))
	tps.metrics.LastUsed = time.Now()
	tps.metrics.AverageLatency = (tps.metrics.AverageLatency*time.Duration(tps.metrics.SuccessfulPartitions-1) + 
		time.Since(start)) / time.Duration(tps.metrics.SuccessfulPartitions)
	
	return plan, nil
}

// HybridParallelismStrategy combines pipeline and tensor parallelism
type HybridParallelismStrategy struct {
	metrics *StrategyMetrics
}

// NewHybridParallelismStrategy creates a new hybrid parallelism strategy
func NewHybridParallelismStrategy() *HybridParallelismStrategy {
	return &HybridParallelismStrategy{
		metrics: &StrategyMetrics{
			LastUsed: time.Now(),
		},
	}
}

// GetName returns the strategy name
func (hps *HybridParallelismStrategy) GetName() string {
	return "hybrid_parallel"
}

// GetMetrics returns strategy metrics
func (hps *HybridParallelismStrategy) GetMetrics() *StrategyMetrics {
	return hps.metrics
}

// CanHandle checks if this strategy can handle the task
func (hps *HybridParallelismStrategy) CanHandle(task *PartitionTask) bool {
	// This strategy works for large models with both many layers and large context
	layerCount := 0
	if task.GGML != nil {
		kv := task.GGML.KV()
		if layers := kv.Uint("llm.layers"); layers > 0 {
			layerCount = int(layers)
		}
	}
	
	// Get context length
	contextLength := 0
	if task.Options.NumCtx > 0 {
		contextLength = task.Options.NumCtx
	} else if optsMap, ok := task.Options.Options.(map[string]interface{}); ok {
		if ctxLen, exists := optsMap["num_ctx"]; exists {
			if ctxLenInt, ok := ctxLen.(int); ok {
				contextLength = ctxLenInt
			}
		}
	}
	
	if contextLength == 0 {
		contextLength = 2048 // Default context length
	}
	
	return layerCount > 20 && contextLength > 2048
}

// Partition implements hybrid parallelism partitioning
func (hps *HybridParallelismStrategy) Partition(ctx context.Context, task *PartitionTask) (*PartitionPlan, error) {
	start := time.Now()
	
	// Get number of layers
	layerCount := 0
	if task.GGML != nil {
		kv := task.GGML.KV()
		if layers := kv.Uint("llm.layers"); layers > 0 {
			layerCount = int(layers)
		}
	}
	
	if layerCount == 0 {
		return nil, fmt.Errorf("unable to determine layer count")
	}
	
	// Get context length
	contextLength := 0
	if task.Options.NumCtx > 0 {
		contextLength = task.Options.NumCtx
	} else if optsMap, ok := task.Options.Options.(map[string]interface{}); ok {
		if ctxLen, exists := optsMap["num_ctx"]; exists {
			if ctxLenInt, ok := ctxLen.(int); ok {
				contextLength = ctxLenInt
			}
		}
	}
	
	if contextLength == 0 {
		contextLength = 2048 // Default context length
	}
	
	nodeCount := len(task.Nodes)
	if nodeCount == 0 {
		return nil, fmt.Errorf("no nodes available")
	}
	
	// For hybrid approach, we divide nodes into pipeline stages
	// and split context within each stage
	
	// Determine pipeline stages (sqrt of nodes for balanced approach)
	pipelineStages := int(math.Sqrt(float64(nodeCount)))
	if pipelineStages < 2 {
		pipelineStages = 2
	}
	if pipelineStages > layerCount {
		pipelineStages = layerCount
	}
	
	nodesPerStage := nodeCount / pipelineStages
	if nodesPerStage == 0 {
		nodesPerStage = 1
	}
	
	// Calculate layers per stage
	layersPerStage := layerCount / pipelineStages
	if layersPerStage == 0 {
		layersPerStage = 1
	}
	
	partitions := make([]*Partition, 0)
	stageIndex := 0
	
	// Create pipeline stages
	for i := 0; i < layerCount; i += layersPerStage {
		endLayer := i + layersPerStage
		if endLayer > layerCount {
			endLayer = layerCount
		}
		
		// For each pipeline stage, split context across nodes in that stage
		stageNodes := make([]*NodeInfo, 0)
		for j := 0; j < nodesPerStage && (stageIndex*nodesPerStage+j) < nodeCount; j++ {
			nodeIndex := stageIndex*nodesPerStage + j
			if nodeIndex < len(task.Nodes) {
				stageNodes = append(stageNodes, task.Nodes[nodeIndex])
			}
		}
		
		if len(stageNodes) == 0 {
			continue
		}
		
		// Split context across nodes in this stage
		contextPerNode := contextLength / len(stageNodes)
		remainder := contextLength % len(stageNodes)
		
		for j, node := range stageNodes {
			startToken := j * contextPerNode
			endToken := startToken + contextPerNode
			if j < remainder {
				startToken += j
				endToken += j + 1
			} else {
				startToken += remainder
				endToken += remainder
			}
			
			partition := &Partition{
				ID:     fmt.Sprintf("partition_%d_hybrid_%d_%d", task.ID, stageIndex, j),
				NodeID: node.ID,
				Type:   PartitionTypeLayer,
				Data: map[string]interface{}{
					"start_layer":  i,
					"end_layer":    endLayer,
					"layer_count":  endLayer - i,
					"start_token":  startToken,
					"end_token":    endToken,
					"token_count":  endToken - startToken,
				},
				Dependencies: []string{}, // Will be set later
				Metadata: map[string]interface{}{
					"pipeline_stage": stageIndex,
					"tensor_split":   j,
				},
				EstimatedLatency: time.Duration((endLayer-i)*(endToken-startToken)*2) * time.Millisecond,
				EstimatedMemory:  int64((endLayer - i) * (endToken - startToken) * 2 * 1024), // Rough estimate
			}
			
			// Set dependencies (depends on previous pipeline stage)
			if stageIndex > 0 {
				// Depend on all partitions from previous stage
				for k := 0; k < len(stageNodes); k++ {
					partition.Dependencies = append(partition.Dependencies, 
						fmt.Sprintf("partition_%d_hybrid_%d_%d", task.ID, stageIndex-1, k))
				}
			}
			
			partitions = append(partitions, partition)
		}
		
		stageIndex++
	}
	
	// Create plan
	plan := &PartitionPlan{
		ID:           fmt.Sprintf("plan_%s", task.ID),
		Strategy:     hps.GetName(),
		Partitions:   partitions,
		Metadata:     make(map[string]interface{}),
		CreatedAt:    time.Now(),
		EstimatedLatency: time.Duration(layerCount*contextLength*2) * time.Millisecond / time.Duration(nodeCount),
		EstimatedThroughput: float64(nodeCount), // Placeholder
		OptimizationScore: 0.9,                 // High score for hybrid approach
	}
	
	// Update metrics
	hps.metrics.TotalPartitions += int64(len(partitions))
	hps.metrics.SuccessfulPartitions += int64(len(partitions))
	hps.metrics.LastUsed = time.Now()
	hps.metrics.AverageLatency = (hps.metrics.AverageLatency*time.Duration(hps.metrics.SuccessfulPartitions-1) + 
		time.Since(start)) / time.Duration(hps.metrics.SuccessfulPartitions)
	
	return plan, nil
}

// AdaptivePartitioningStrategy adapts partitioning based on workload characteristics
type AdaptivePartitioningStrategy struct {
	metrics    *StrategyMetrics
	history    []*PartitionResult
	thresholds map[string]float64
}

// NewAdaptivePartitioningStrategy creates a new adaptive partitioning strategy
func NewAdaptivePartitioningStrategy() *AdaptivePartitioningStrategy {
	return &AdaptivePartitioningStrategy{
		metrics: &StrategyMetrics{
			LastUsed: time.Now(),
		},
		history: make([]*PartitionResult, 0),
		thresholds: map[string]float64{
			"large_model":     5.0 * 1024 * 1024 * 1024, // 5GB
			"large_context":   2048,
			"many_layers":     20,
			"high_parallelism": 0.8,
		},
	}
}

// GetName returns the strategy name
func (aps *AdaptivePartitioningStrategy) GetName() string {
	return "adaptive"
}

// GetMetrics returns strategy metrics
func (aps *AdaptivePartitioningStrategy) GetMetrics() *StrategyMetrics {
	return aps.metrics
}

// CanHandle checks if this strategy can handle the task
func (aps *AdaptivePartitioningStrategy) CanHandle(task *PartitionTask) bool {
	// This strategy can handle any task
	return true
}

// Partition implements adaptive partitioning based on workload analysis
func (aps *AdaptivePartitioningStrategy) Partition(ctx context.Context, task *PartitionTask) (*PartitionPlan, error) {
	start := time.Now()
	
	// Analyze workload characteristics
	modelSize := aps.estimateModelSize(task)
	
	// Get context length
	contextLength := 0
	if task.Options.NumCtx > 0 {
		contextLength = task.Options.NumCtx
	} else if optsMap, ok := task.Options.Options.(map[string]interface{}); ok {
		if ctxLen, exists := optsMap["num_ctx"]; exists {
			if ctxLenInt, ok := ctxLen.(int); ok {
				contextLength = ctxLenInt
			}
		}
	}
	
	if contextLength == 0 {
		contextLength = 2048 // Default context length
	}
	
	layerCount := aps.estimateLayerCount(task)
	parallelizability := aps.estimateParallelizability(task)
	nodeCount := len(task.Nodes)
	
	// Select the best strategy based on workload analysis
	var plan *PartitionPlan
	var err error
	
	// For very large models, use pipeline parallelism
	if modelSize > aps.thresholds["large_model"] && layerCount > int(aps.thresholds["many_layers"]) {
		strategy := NewPipelineParallelismStrategy()
		plan, err = strategy.Partition(ctx, task)
	} else if contextLength > int(aps.thresholds["large_context"]) && parallelizability > aps.thresholds["high_parallelism"] {
		// For large context with high parallelizability, use tensor parallelism
		strategy := NewTensorParallelismStrategy()
		plan, err = strategy.Partition(ctx, task)
	} else if modelSize > aps.thresholds["large_model"] && contextLength > int(aps.thresholds["large_context"]) {
		// For both large model and large context, use hybrid parallelism
		strategy := NewHybridParallelismStrategy()
		plan, err = strategy.Partition(ctx, task)
	} else if nodeCount > 1 && layerCount > int(aps.thresholds["many_layers"]) {
		// For multi-node setups with sufficient layers, use pipeline parallelism
		strategy := NewPipelineParallelismStrategy()
		plan, err = strategy.Partition(ctx, task)
	} else {
		// Default to layerwise partitioning
		strategy := NewLayerwiseStrategy()
		plan, err = strategy.Partition(ctx, task)
	}
	
	if err != nil {
		aps.metrics.FailedPartitions++
		return nil, err
	}
	
	// Update metrics
	aps.metrics.TotalPartitions += int64(len(plan.Partitions))
	aps.metrics.SuccessfulPartitions += int64(len(plan.Partitions))
	aps.metrics.LastUsed = time.Now()
	aps.metrics.AverageLatency = (aps.metrics.AverageLatency*time.Duration(aps.metrics.SuccessfulPartitions-1) + 
		time.Since(start)) / time.Duration(aps.metrics.SuccessfulPartitions)
	
	// Record result for learning
	result := &PartitionResult{
		Plan:             plan,
		ActualLatency:    time.Since(start),
		ActualThroughput: 1.0, // Placeholder
		Success:          true,
		Timestamp:        time.Now(),
	}
	aps.recordResult(result)
	
	return plan, nil
}

// recordResult records a partitioning result for learning
func (aps *AdaptivePartitioningStrategy) recordResult(result *PartitionResult) {
	aps.history = append(aps.history, result)
	
	// Keep only last 1000 results
	if len(aps.history) > 1000 {
		aps.history = aps.history[len(aps.history)-1000:]
	}
	
	// Adjust thresholds based on performance
	aps.adjustThresholds()
}

// adjustThresholds adjusts thresholds based on historical performance
func (aps *AdaptivePartitioningStrategy) adjustThresholds() {
	if len(aps.history) < 10 {
		return // Not enough data
	}
	
	// Analyze recent performance
	recentResults := aps.history[len(aps.history)-10:]
	
	// Calculate average performance metrics
	totalLatency := time.Duration(0)
	totalThroughput := 0.0
	
	for _, result := range recentResults {
		totalLatency += result.ActualLatency
		totalThroughput += result.ActualThroughput
	}
	
	avgLatency := time.Duration(int64(totalLatency) / int64(len(recentResults)))
	avgThroughput := totalThroughput / float64(len(recentResults))
	
	// Adjust thresholds based on performance
	// If performance is good, we can be more aggressive with partitioning
	if avgLatency < 100*time.Millisecond && avgThroughput > 5.0 {
		// Increase thresholds to enable more aggressive partitioning
		aps.thresholds["large_model"] *= 0.95
		aps.thresholds["large_context"] *= 0.95
		aps.thresholds["many_layers"] *= 0.95
		aps.thresholds["high_parallelism"] *= 0.95
	} else if avgLatency > 500*time.Millisecond || avgThroughput < 1.0 {
		// Decrease thresholds to be more conservative
		aps.thresholds["large_model"] *= 1.05
		aps.thresholds["large_context"] *= 1.05
		aps.thresholds["many_layers"] *= 1.05
		aps.thresholds["high_parallelism"] *= 1.05
	}
	
	// Ensure thresholds stay within reasonable bounds
	aps.ensureThresholdBounds()
}

// ensureThresholdBounds ensures thresholds stay within reasonable bounds
func (aps *AdaptivePartitioningStrategy) ensureThresholdBounds() {
	// Model size: between 1GB and 50GB
	if aps.thresholds["large_model"] < 1.0*1024*1024*1024 {
		aps.thresholds["large_model"] = 1.0 * 1024 * 1024 * 1024
	}
	if aps.thresholds["large_model"] > 50.0*1024*1024*1024 {
		aps.thresholds["large_model"] = 50.0 * 1024 * 1024 * 1024
	}
	
	// Context length: between 512 and 16384
	if aps.thresholds["large_context"] < 512 {
		aps.thresholds["large_context"] = 512
	}
	if aps.thresholds["large_context"] > 16384 {
		aps.thresholds["large_context"] = 16384
	}
	
	// Layer count: between 5 and 100
	if aps.thresholds["many_layers"] < 5 {
		aps.thresholds["many_layers"] = 5
	}
	if aps.thresholds["many_layers"] > 100 {
		aps.thresholds["many_layers"] = 100
	}
	
	// Parallelizability: between 0.1 and 0.95
	if aps.thresholds["high_parallelism"] < 0.1 {
		aps.thresholds["high_parallelism"] = 0.1
	}
	if aps.thresholds["high_parallelism"] > 0.95 {
		aps.thresholds["high_parallelism"] = 0.95
	}
}

// estimateModelSize estimates the size of a model
func (aps *AdaptivePartitioningStrategy) estimateModelSize(task *PartitionTask) float64 {
	if task.GGML != nil {
		return float64(task.GGML.Length)
	}
	// Fallback estimation based on model name patterns
	return 4.0 * 1024 * 1024 * 1024 // 4GB default
}

// estimateLayerCount estimates the number of layers in a model
func (aps *AdaptivePartitioningStrategy) estimateLayerCount(task *PartitionTask) int {
	if task.GGML != nil {
		kv := task.GGML.KV()
		if layers := kv.Uint("llm.layers"); layers > 0 {
			return int(layers)
		}
	}
	// Fallback estimation
	return 24 // Default for many transformer models
}

// estimateParallelizability estimates how parallelizable a task is
func (aps *AdaptivePartitioningStrategy) estimateParallelizability(task *PartitionTask) float64 {
	// Get context length
	contextLength := 0
	if task.Options.NumCtx > 0 {
		contextLength = task.Options.NumCtx
	} else if optsMap, ok := task.Options.Options.(map[string]interface{}); ok {
		if ctxLen, exists := optsMap["num_ctx"]; exists {
			if ctxLenInt, ok := ctxLen.(int); ok {
				contextLength = ctxLenInt
			}
		}
	}
	
	if contextLength == 0 {
		contextLength = 2048 // Default context length
	}
	
	// Factors that affect parallelizability:
	// 1. Model architecture (transformers are more parallelizable)
	// 2. Context length (longer contexts are more parallelizable)
	// 3. Batch size (larger batches are more parallelizable)
	
	// Base parallelizability on context length
	parallelizability := math.Min(float64(contextLength)/2048.0, 1.0)
	
	// Adjust based on model type
	if task.Model != nil {
		// Check if model is a transformer (more parallelizable)
		if isTransformerModel(task.Model) {
			parallelizability *= 1.2
		}
	}
	
	return math.Min(parallelizability, 1.0)
}

// Update the PartitionManager to register the new strategies
func (pm *PartitionManager) registerAdvancedStrategies() {
	pm.RegisterStrategy(NewPipelineParallelismStrategy())
	pm.RegisterStrategy(NewTensorParallelismStrategy())
	pm.RegisterStrategy(NewHybridParallelismStrategy())
	pm.RegisterStrategy(NewAdaptivePartitioningStrategy())
}